{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5437a8ee-d6a4-4207-9306-a1f3e5e6aef4",
   "metadata": {},
   "source": [
    "# Financial News Sentiment Analysis Tool\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Python tool analyzes the sentiment of financial news articles using Google's Gemini API. It processes news articles from JSON files, extracts sentiment scores across multiple dimensions, and outputs the results to CSV files for further analysis.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Multi-dimensional sentiment analysis using Gemini AI\n",
    "- Rate limiting to comply with API constraints\n",
    "- Batch processing for efficient handling of large datasets\n",
    "- Progress tracking to allow for interrupted processing to resume\n",
    "- Date filtering to focus on specific time periods\n",
    "- Comprehensive sentiment metrics including:\n",
    "  - Basic polarity detection\n",
    "  - Fine-grained sentiment analysis\n",
    "  - Aspect-based sentiment analysis for financial dimensions\n",
    "  - Topic-sentiment analysis\n",
    "  - Emotion detection\n",
    "  - Intent analysis\n",
    "  - Subjectivity/objectivity detection\n",
    "  - Contextual sentiment analysis\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.8+\n",
    "- Google Gemini API key\n",
    "- Required Python packages:\n",
    "  - pandas\n",
    "  - google-generativeai\n",
    "  - tqdm\n",
    "  - Other standard libraries (json, time, os, logging, glob, re, etc.)\n",
    "\n",
    "## Installation\n",
    "\n",
    "1. Install required packages:\n",
    "   ```bash\n",
    "   pip install pandas tqdm google-generativeai\n",
    "   ```\n",
    "\n",
    "2. Set up your Gemini API key:\n",
    "   - Replace the placeholder API key in the code with your actual key\n",
    "   - Or set it as an environment variable (modify the code accordingly)\n",
    "\n",
    "3. Prepare your directory structure:\n",
    "   ```\n",
    "   project_root/\n",
    "   ├── data/\n",
    "   │   └── news_data/           # Where news JSON files are stored\n",
    "   ├── log/\n",
    "   │   └── sentiment/\n",
    "   │       └── news/            # Where logs will be saved\n",
    "   ├── progress/\n",
    "   │   └── news/                # Where progress tracking files will be saved\n",
    "   └── sentiment_results/\n",
    "       └── news/                # Where results will be saved\n",
    "   ```\n",
    "\n",
    "## Input Data Format\n",
    "\n",
    "The tool expects JSON files in the `data/news_data/` directory with naming convention `[TICKER]_news.json`. Each JSON file should contain an array of news articles with at least the following fields:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"title\": \"Article title\",\n",
    "    \"content\": \"Full article content...\",\n",
    "    \"date\": \"YYYY-MM-DD\",\n",
    "    \"link\": \"URL to the original article\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Run the script directly:\n",
    "\n",
    "```bash\n",
    "python sentiment_analysis.py\n",
    "```\n",
    "\n",
    "The script will prompt you for:\n",
    "- Batch size (how many articles to process at once)\n",
    "- Whether to force reprocessing of already processed articles\n",
    "- Number of rows to process per file (for testing or limiting processing)\n",
    "\n",
    "To integrate the tool into another script:\n",
    "\n",
    "```python\n",
    "from sentiment_analysis import process_multiple_news_files\n",
    "\n",
    "# Process all news files with custom settings\n",
    "process_multiple_news_files(\n",
    "    num_rows=100,             # Process only 100 rows per file (None for all)\n",
    "    force_reprocess=False,    # Skip already processed items\n",
    "    batch_size=20             # Process 20 items at a time\n",
    ")\n",
    "\n",
    "# Process specific files\n",
    "files = [\"data/news_data/AAPL_news.json\", \"data/news_data/MSFT_news.json\"]\n",
    "process_multiple_news_files(\n",
    "    files=files,\n",
    "    num_rows=None,\n",
    "    force_reprocess=True,\n",
    "    batch_size=10\n",
    ")\n",
    "```\n",
    "\n",
    "## Output Format\n",
    "\n",
    "The tool generates CSV files in the `sentiment_results/news/` directory with naming convention `news_sentiment_[TICKER].csv`. Each CSV file contains:\n",
    "\n",
    "- Basic article information (ticker, date, title, link)\n",
    "- Various sentiment scores, including:\n",
    "  - Overall polarity (-1 to 1)\n",
    "  - Emotion and intent scores\n",
    "  - Average scores for fine-grained, aspect-based, topic, and contextual sentiment\n",
    "  - Individual aspect scores (financial performance, company operations, etc.)\n",
    "  - Individual topic scores\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "Key constants that can be modified in the code:\n",
    "\n",
    "- `MAX_REQUESTS_PER_MINUTE`: API rate limit (default: 1900)\n",
    "- `MAX_REQUESTS_PER_DAY`: API daily limit (default: 1000000)\n",
    "- `REQUEST_DELAY`: Minimum delay between requests (default: 0.5 seconds)\n",
    "- `MAX_TEXT_LENGTH`: Maximum length for text inputs (default: 60000 characters)\n",
    "- `MIN_DATE` and `MAX_DATE`: Date range for filtering news (default: 2023-01-01 to 2024-12-31)\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "### Main Components\n",
    "\n",
    "#### RateLimiter Class\n",
    "Manages API rate limiting to prevent quota issues.\n",
    "\n",
    "#### Key Functions\n",
    "\n",
    "- `analyze_sentiment_with_gemini(text)`: Sends text to Gemini API and retrieves sentiment analysis\n",
    "- `extract_json_from_response(response_text)`: Parses JSON from API response\n",
    "- `flatten_sentiment_json(json_obj)`: Converts nested JSON to flat dictionary for CSV output\n",
    "- `analyze_sentiment_batch(batch_items, filtered_news, retry_limit)`: Processes a batch of articles\n",
    "- `process_news_file(file_path, ...)`: Processes all articles in a single news file\n",
    "- `process_multiple_news_files(files, ...)`: Processes multiple news files\n",
    "\n",
    "### Sentiment Analysis Prompt\n",
    "\n",
    "The tool uses a detailed prompt that instructs the Gemini API to analyze sentiment across multiple dimensions:\n",
    "\n",
    "1. Polarity Detection: Overall positive/negative sentiment\n",
    "2. Fine-Grained Sentiment Analysis: Individual scores for different sections\n",
    "3. Aspect-Based Sentiment Analysis: Scores for financial aspects (performance, operations, etc.)\n",
    "4. Topic-Sentiment Analysis: Scores for key topics\n",
    "5. Emotion Detection: Dominant emotions in the article\n",
    "6. Intent Analysis: Purpose of the article\n",
    "7. Subjectivity/Objectivity Detection: How factual vs. opinionated the article is\n",
    "8. Contextual Sentiment Analysis: Sentiment across different contexts\n",
    "9. Deep Learning-Based Analysis: Gemini's own sentiment assessment\n",
    "\n",
    "### Error Handling and Retry Logic\n",
    "\n",
    "The tool includes robust error handling:\n",
    "- Failed API calls are retried up to a configurable limit\n",
    "- JSON parsing errors are handled gracefully\n",
    "- Progress is saved after each batch to prevent data loss\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Dependent on Gemini API availability and quotas\n",
    "- Processing large volumes of news can be time-consuming\n",
    "- API costs can accumulate with large datasets\n",
    "- Sentiment analysis quality depends on the Gemini model's capabilities\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **API Key Issues**: Ensure your API key is valid and has sufficient quota\n",
    "- **Rate Limiting**: Adjust the rate limits if you encounter API throttling\n",
    "- **Memory Issues**: For very large datasets, consider reducing batch size\n",
    "- **JSON Parsing Errors**: Check that the API responses conform to expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92466e2d-992a-42b7-a5f3-6c0ddccef35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "\n",
    "# Set up logging\n",
    "log_dir = \"log/sentiment/news\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'{log_dir}/gemini_news_sentiment_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Gemini API configuration\n",
    "API_KEY = \"AIzaSyAyPLiF-ckAV2N81bNwUZPzk1Vrrs-R9MI\"  # Replace with your actual API key\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "# Set up Gemini client\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "# Constants for API limits and processing\n",
    "MAX_REQUESTS_PER_MINUTE = 1900  # Using your specified rate limit\n",
    "MAX_REQUESTS_PER_DAY = 1000000  # Using your specified daily limit\n",
    "REQUEST_DELAY = 0.5  # Using your specified delay\n",
    "SAVE_FREQUENCY = 5  # Using your specified save frequency\n",
    "MAX_TEXT_LENGTH = 60000  # Maximum length for text inputs\n",
    "MIN_DATE = \"2023-01-01\"  # Start date for filtering news\n",
    "MAX_DATE = \"2024-12-31\"  # End date for filtering news\n",
    "\n",
    "# Create directories for outputs and progress tracking\n",
    "os.makedirs(\"sentiment_results/news\", exist_ok=True)\n",
    "os.makedirs(\"progress/news\", exist_ok=True)\n",
    "\n",
    "# Class to track and enforce rate limits\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_per_minute, max_per_day):\n",
    "        self.max_per_minute = max_per_minute\n",
    "        self.max_per_day = max_per_day\n",
    "        self.minute_requests = deque()\n",
    "        self.daily_requests = 0\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def check_and_wait(self):\n",
    "        \"\"\"Check if we can make a request, wait if needed, and track the request\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Check daily limit\n",
    "        if self.daily_requests >= self.max_per_day:\n",
    "            logger.warning(f\"Reached maximum daily request limit of {self.max_per_day}\")\n",
    "            return False\n",
    "        \n",
    "        # Clean up minute_requests older than 60 seconds\n",
    "        while self.minute_requests and current_time - self.minute_requests[0] > 60:\n",
    "            self.minute_requests.popleft()\n",
    "        \n",
    "        # Check if we're at the per-minute limit\n",
    "        if len(self.minute_requests) >= self.max_per_minute:\n",
    "            wait_time = 60 - (current_time - self.minute_requests[0])\n",
    "            if wait_time > 0:\n",
    "                logger.info(f\"Rate limit approaching: Waiting {wait_time:.2f} seconds before next request\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        # Always wait the minimum delay between requests\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        # Record this request\n",
    "        self.minute_requests.append(time.time())\n",
    "        self.daily_requests += 1\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Create rate limiter instance\n",
    "rate_limiter = RateLimiter(MAX_REQUESTS_PER_MINUTE, MAX_REQUESTS_PER_DAY)\n",
    "\n",
    "# Helper function to check if a row has already been processed\n",
    "def get_processed_indices(progress_file):\n",
    "    \"\"\"Load the set of indices that have already been processed\"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "\n",
    "# Helper function to save processed indices\n",
    "def save_processed_indices(progress_file, processed_indices):\n",
    "    \"\"\"Save the set of indices that have been processed\"\"\"\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(list(processed_indices), f)\n",
    "\n",
    "# Helper function to clean and standardize dates\n",
    "def clean_date(date_str):\n",
    "    \"\"\"Extract YYYY-MM-DD from date string\"\"\"\n",
    "    try:\n",
    "        # Parse the date string to datetime\n",
    "        dt = pd.to_datetime(date_str)\n",
    "        # Return only the date part as string\n",
    "        return dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        logger.warning(f\"Could not parse date: {date_str}\")\n",
    "        return None\n",
    "\n",
    "# Function to analyze sentiment via Gemini API with rate limiting\n",
    "def analyze_sentiment_with_gemini(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of a single text using Gemini API\n",
    "    \"\"\"\n",
    "    # Using the EXACT prompt as provided by you\n",
    "    prompt = f\"\"\"\n",
    "Analyze the following financial news article for sentiment using multiple methods. Provide sentiment scores between -1 (strongly negative) and 1 (strongly positive) for each method.\n",
    "\n",
    "Financial News Article: {text}\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1.  Focus on the direct relevance of the news to the company mentioned in the article.\n",
    "2.  Consider the article's impact on the company's stock price or financial performance.\n",
    "3.  Pay attention to any forward-looking statements or projections.\n",
    "4.  Identify and analyze any specific events or announcements mentioned in the article.\n",
    "\n",
    "Methods:\n",
    "\n",
    "1.  Polarity Detection (Basic Sentiment): Provide a single score representing the overall positive, negative, or neutral sentiment of the news article in relation to the company.\n",
    "2.  Fine-Grained Sentiment Analysis: Provide individual scores for different sections or perspectives within the article, if applicable, and an average score.\n",
    "3.  Aspect-Based Sentiment Analysis (ABSA): Identify the key aspects or attributes that are discussed in the news article that relates to the company, and provide sentiment scores for each identified aspect, and an average score. If possible, prioritize the following aspects:\n",
    "    * Financial Performance: Earnings/Revenue, Profitability/Margins, Financial Health\n",
    "    * Company Operations: Product/Service Developments, Sales/Market Share, Operational Efficiency\n",
    "    * Growth and Strategy: Expansion/Acquisitions, Strategic Initiatives, Future Outlook/Guidance\n",
    "    * Market/Industry: Industry Trends, Competitive Landscape, Regulatory Changes\n",
    "    * Management/Leadership: Executive Changes, Management Decisions, Corporate Governance\n",
    "    * Stock/Valuation: Stock Price Movements, Analyst Ratings/Targets, Valuation Metrics\n",
    "    * Risks/Challenges: Legal Issues/Litigation, Financial Risks, Reputational Risks\n",
    "4.  Topic-Sentiment Analysis: Identify the key topics or themes that are discussed in the news article that relates to the company, and provide sentiment scores for each identified topic, and an average score. If possible, prioritize the following topics:\n",
    "    * Financial Performance\n",
    "    * Company Operations\n",
    "    * Growth and Strategy\n",
    "    * Market/Industry\n",
    "    * Management/Leadership\n",
    "    * Stock/Valuation\n",
    "    * Risks/Challenges\n",
    "5.  Emotion Detection: Provide a score representing the dominant emotion(s) expressed in the news article. If multiple emotions are present, provide a weighted average.\n",
    "6.  Intent Analysis: Provide a score representing the overall intent or purpose of the news article. If there are multiple intents, provide an average score.\n",
    "7.  Subjectivity/Objectivity Detection: Provide a score representing the overall level of subjectivity or objectivity in the news article.\n",
    "8.  Contextual Sentiment Analysis: Provide individual scores for different contextual elements or speakers within the news article, and an average score.\n",
    "9.  Deep Learning-Based Approach (Gemini's Analysis): Provide overall, emotional, and contextual sentiment scores using Gemini's advanced analysis.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{{\n",
    "  \"polarity_detection\": score,\n",
    "  \"fine_grained_sentiment\": {{\n",
    "    \"individual_scores\": [{{\"section/perspective\": score}}],\n",
    "    \"average_score\": score\n",
    "  }},\n",
    "  \"aspect_based_sentiment\": {{\n",
    "    \"aspect_scores\": [{{\"aspect\": score}}],\n",
    "    \"average_score\": score\n",
    "  }},\n",
    "  \"topic_sentiment_analysis\": {{\n",
    "    \"topic_scores\": [{{\"topic\": score}}],\n",
    "    \"average_score\": score\n",
    "  }},\n",
    "  \"emotion_detection\": score,\n",
    "  \"intent_analysis\": score,\n",
    "  \"subjectivity_objectivity\": score,\n",
    "  \"contextual_sentiment\": {{\n",
    "    \"context_scores\": [{{\"context_element\": score}}],\n",
    "    \"average_score\": score\n",
    "  }},\n",
    "  \"gemini_analysis\": {{\n",
    "    \"overall_sentiment\": score,\n",
    "    \"emotional_sentiment\": score,\n",
    "    \"contextual_sentiment\": score\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Check rate limits\n",
    "        if not rate_limiter.check_and_wait():\n",
    "            logger.warning(\"Rate limit reached. Skipping request.\")\n",
    "            return None\n",
    "\n",
    "        # Use Gemini API\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt\n",
    "            )\n",
    "            \n",
    "            # Safety checks and handling\n",
    "            if not hasattr(response, 'text'):\n",
    "                logger.error(\"No text attribute in response\")\n",
    "                return None\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Gemini API call: {e}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing sentiment analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Extract the JSON object from the API response text using a more robust method\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Look for JSON pattern using regex - this is more reliable\n",
    "        match = re.search(r'({.*})', response_text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            # Clean up potential issues before parsing\n",
    "            json_str = re.sub(r',\\s*}', '}', json_str)  # Remove trailing commas\n",
    "            json_str = re.sub(r',\\s*]', ']', json_str)  # Remove trailing commas in arrays\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            logger.error(\"No JSON pattern found in response\")\n",
    "            return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error parsing JSON: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error extracting JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "def flatten_sentiment_json(json_obj):\n",
    "    \"\"\"\n",
    "    Flatten the nested JSON structure into a dictionary with consistent key-value pairs\n",
    "    Only keeps predefined scores and categories for consistent CSV structure\n",
    "    \"\"\"\n",
    "    flat_dict = {}\n",
    "    \n",
    "    if not json_obj:\n",
    "        return flat_dict\n",
    "    \n",
    "    # Add default values for key metrics\n",
    "    # Basic scores\n",
    "    flat_dict[\"polarity_detection\"] = 0.0\n",
    "    flat_dict[\"emotion_detection\"] = 0.0\n",
    "    flat_dict[\"intent_analysis\"] = 0.0\n",
    "    flat_dict[\"subjectivity_objectivity\"] = 0.0\n",
    "    \n",
    "    # Average scores\n",
    "    flat_dict[\"fine_grained_sentiment_avg\"] = 0.0\n",
    "    flat_dict[\"aspect_based_sentiment_avg\"] = 0.0\n",
    "    flat_dict[\"topic_sentiment_analysis_avg\"] = 0.0\n",
    "    flat_dict[\"contextual_sentiment_avg\"] = 0.0\n",
    "    \n",
    "    # Gemini analysis scores\n",
    "    flat_dict[\"gemini_overall_sentiment\"] = 0.0\n",
    "    flat_dict[\"gemini_emotional_sentiment\"] = 0.0\n",
    "    flat_dict[\"gemini_contextual_sentiment\"] = 0.0\n",
    "    \n",
    "    # Predefined aspects (initialize with default values)\n",
    "    predefined_aspects = [\n",
    "        \"financial_performance\",\n",
    "        \"company_operations\",\n",
    "        \"growth_and_strategy\",\n",
    "        \"market_industry\",\n",
    "        \"management_leadership\",\n",
    "        \"stock_valuation\",\n",
    "        \"risks_challenges\"\n",
    "    ]\n",
    "    \n",
    "    for aspect in predefined_aspects:\n",
    "        flat_dict[f\"aspect_{aspect}\"] = 0.0\n",
    "    \n",
    "    # Predefined topics (same as aspects for this case)\n",
    "    for topic in predefined_aspects:\n",
    "        flat_dict[f\"topic_{topic}\"] = 0.0\n",
    "    \n",
    "    # Extract top-level simple scores\n",
    "    for key in ['polarity_detection', 'emotion_detection', 'intent_analysis', 'subjectivity_objectivity']:\n",
    "        if key in json_obj:\n",
    "            try:\n",
    "                value = json_obj[key]\n",
    "                if isinstance(value, list) and len(value) > 0:\n",
    "                    value = value[0]\n",
    "                flat_dict[key] = float(value)\n",
    "            except Exception:\n",
    "                pass  # Keep default if error\n",
    "    \n",
    "    # Extract Gemini Analysis scores\n",
    "    if 'gemini_analysis' in json_obj:\n",
    "        for sub_key, value in json_obj['gemini_analysis'].items():\n",
    "            try:\n",
    "                if isinstance(value, list) and len(value) > 0:\n",
    "                    value = value[0]\n",
    "                flat_dict[f\"gemini_{sub_key}\"] = float(value)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Extract average scores only (no individual sections)\n",
    "    if 'fine_grained_sentiment' in json_obj and 'average_score' in json_obj['fine_grained_sentiment']:\n",
    "        try:\n",
    "            value = json_obj['fine_grained_sentiment']['average_score']\n",
    "            if isinstance(value, list) and len(value) > 0:\n",
    "                value = value[0]\n",
    "            flat_dict[\"fine_grained_sentiment_avg\"] = float(value)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Extract other average scores\n",
    "    for key in ['aspect_based_sentiment', 'topic_sentiment_analysis', 'contextual_sentiment']:\n",
    "        if key in json_obj and 'average_score' in json_obj[key]:\n",
    "            try:\n",
    "                value = json_obj[key]['average_score']\n",
    "                if isinstance(value, list) and len(value) > 0:\n",
    "                    value = value[0]\n",
    "                flat_dict[f\"{key}_avg\"] = float(value)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Process predefined aspects only\n",
    "    if 'aspect_based_sentiment' in json_obj and 'aspect_scores' in json_obj['aspect_based_sentiment']:\n",
    "        aspect_scores = json_obj['aspect_based_sentiment']['aspect_scores']\n",
    "        if isinstance(aspect_scores, list):\n",
    "            for score_dict in aspect_scores:\n",
    "                for aspect, score in score_dict.items():\n",
    "                    # Normalize aspect name for matching\n",
    "                    normalized_aspect = aspect.lower().replace('/', '_').replace(' ', '_').replace(':', '_')\n",
    "                    \n",
    "                    # Find matching predefined aspect\n",
    "                    matched_aspect = None\n",
    "                    for predefined in predefined_aspects:\n",
    "                        if predefined in normalized_aspect:\n",
    "                            matched_aspect = predefined\n",
    "                            break\n",
    "                    \n",
    "                    # Store only if it matches a predefined aspect\n",
    "                    if matched_aspect:\n",
    "                        try:\n",
    "                            if isinstance(score, list) and len(score) > 0:\n",
    "                                score = score[0]\n",
    "                            flat_dict[f\"aspect_{matched_aspect}\"] = float(score)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "    \n",
    "    # Process predefined topics only (similar to aspects)\n",
    "    if 'topic_sentiment_analysis' in json_obj and 'topic_scores' in json_obj['topic_sentiment_analysis']:\n",
    "        topic_scores = json_obj['topic_sentiment_analysis']['topic_scores']\n",
    "        if isinstance(topic_scores, list):\n",
    "            for score_dict in topic_scores:\n",
    "                for topic, score in score_dict.items():\n",
    "                    # Normalize topic name for matching\n",
    "                    normalized_topic = topic.lower().replace('/', '_').replace(' ', '_')\n",
    "                    \n",
    "                    # Find matching predefined topic\n",
    "                    matched_topic = None\n",
    "                    for predefined in predefined_aspects:  # Using same list as aspects\n",
    "                        if predefined in normalized_topic:\n",
    "                            matched_topic = predefined\n",
    "                            break\n",
    "                    \n",
    "                    # Store only if it matches a predefined topic\n",
    "                    if matched_topic:\n",
    "                        try:\n",
    "                            if isinstance(score, list) and len(score) > 0:\n",
    "                                score = score[0]\n",
    "                            flat_dict[f\"topic_{matched_topic}\"] = float(score)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "    \n",
    "    return flat_dict\n",
    "\n",
    "# New function to process batches of articles\n",
    "def analyze_sentiment_batch(batch_items, filtered_news, retry_limit=3):\n",
    "    \"\"\"\n",
    "    Process a batch of news items\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch_items : list\n",
    "        List of indices to process\n",
    "    filtered_news : list\n",
    "        List of news items\n",
    "    retry_limit : int\n",
    "        Number of retries for API calls\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping indices to sentiment results\n",
    "    \"\"\"\n",
    "    batch_results = {}\n",
    "    \n",
    "    for idx in batch_items:\n",
    "        news_item = filtered_news[idx]\n",
    "        \n",
    "        # Combine title and content for analysis\n",
    "        title = news_item.get('title', '')\n",
    "        content = news_item.get('content', '')\n",
    "        \n",
    "        if not content or not isinstance(content, str):\n",
    "            logger.warning(f\"Empty content for news item {idx}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Combine title and content for more comprehensive analysis\n",
    "        combined_text = f\"Title: {title}\\n\\nContent: {content}\"\n",
    "        \n",
    "        # Limit text length if too long\n",
    "        if len(combined_text) > MAX_TEXT_LENGTH:\n",
    "            combined_text = combined_text[:MAX_TEXT_LENGTH]\n",
    "        \n",
    "        # Retry mechanism\n",
    "        success = False\n",
    "        for attempt in range(retry_limit):\n",
    "            try:\n",
    "                # Call Gemini API\n",
    "                response_text = analyze_sentiment_with_gemini(combined_text)\n",
    "                \n",
    "                if not response_text:\n",
    "                    logger.error(f\"No response from API for news item {idx}, attempt {attempt+1}/{retry_limit}\")\n",
    "                    time.sleep(3)  # Wait before retry\n",
    "                    continue\n",
    "                \n",
    "                # Extract JSON from response\n",
    "                sentiment_json = extract_json_from_response(response_text)\n",
    "                \n",
    "                if not sentiment_json:\n",
    "                    logger.error(f\"Failed to extract JSON from response for news item {idx}, attempt {attempt+1}/{retry_limit}\")\n",
    "                    time.sleep(3)  # Wait before retry\n",
    "                    continue\n",
    "                \n",
    "                # Flatten JSON\n",
    "                flat_sentiment = flatten_sentiment_json(sentiment_json)\n",
    "                \n",
    "                # Store results\n",
    "                batch_results[idx] = flat_sentiment\n",
    "                \n",
    "                success = True\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error on attempt {attempt+1}/{retry_limit} for news item {idx}: {e}\")\n",
    "                time.sleep(3)  # Wait before retry\n",
    "        \n",
    "        if not success:\n",
    "            logger.error(f\"Failed to analyze sentiment for news item {idx} after {retry_limit} attempts\")\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def process_news_file(file_path, output_dir=\"sentiment_results/news\", num_rows=None, retry_limit=3, force_reprocess=True, batch_size=10):\n",
    "    \"\"\"\n",
    "    Process a single news JSON file and run sentiment analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the JSON file\n",
    "    output_dir : str\n",
    "        Directory to save results\n",
    "    num_rows : int or None\n",
    "        Number of rows to process (None for all)\n",
    "    retry_limit : int\n",
    "        Number of retries for API calls\n",
    "    force_reprocess : bool\n",
    "        If True, reprocess all rows even if they've been processed before\n",
    "    batch_size : int\n",
    "        Number of news items to process in a batch\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Extract ticker from filename\n",
    "        ticker = os.path.basename(file_path).replace('_news.json', '')\n",
    "        \n",
    "        # Load the JSON file\n",
    "        logger.info(f\"Loading news file: {file_path}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            news_data = json.load(f)\n",
    "            \n",
    "        logger.info(f\"Loaded {len(news_data)} news items for {ticker}\")\n",
    "        \n",
    "        # Filter by date\n",
    "        filtered_news = []\n",
    "        for item in news_data:\n",
    "            clean_date_str = clean_date(item.get('date', ''))\n",
    "            if clean_date_str:\n",
    "                if MIN_DATE <= clean_date_str <= MAX_DATE:\n",
    "                    # Add cleaned date and ticker to the item\n",
    "                    item['clean_date'] = clean_date_str\n",
    "                    item['ticker'] = ticker\n",
    "                    filtered_news.append(item)\n",
    "        \n",
    "        logger.info(f\"Filtered to {len(filtered_news)} news items between {MIN_DATE} and {MAX_DATE}\")\n",
    "        \n",
    "        # Limit rows if specified\n",
    "        if num_rows and num_rows < len(filtered_news):\n",
    "            filtered_news = filtered_news[:num_rows]\n",
    "            logger.info(f\"Limited to first {num_rows} news items for testing\")\n",
    "            \n",
    "        # Set up progress tracking\n",
    "        progress_file = f\"progress/news/news_{ticker}_progress.json\"\n",
    "        os.makedirs(os.path.dirname(progress_file), exist_ok=True)\n",
    "        processed_indices = get_processed_indices(progress_file)\n",
    "        \n",
    "        # Create a list of indices to process\n",
    "        all_indices = list(range(len(filtered_news)))\n",
    "        if not force_reprocess:\n",
    "            # Only process unprocessed items\n",
    "            indices_to_process = [idx for idx in all_indices if idx not in processed_indices]\n",
    "            logger.info(f\"Found {len(indices_to_process)} unprocessed news items\")\n",
    "        else:\n",
    "            indices_to_process = all_indices\n",
    "            logger.info(f\"Force reprocessing all {len(indices_to_process)} news items\")\n",
    "        \n",
    "        # Track results\n",
    "        results = {}\n",
    "        \n",
    "        # Process in batches\n",
    "        total_batches = (len(indices_to_process) + batch_size - 1) // batch_size\n",
    "        logger.info(f\"Processing {len(indices_to_process)} news items in {total_batches} batches of size {batch_size}\")\n",
    "        \n",
    "        for batch_idx in tqdm(range(0, len(indices_to_process), batch_size), desc=f\"Processing {ticker} news batches\"):\n",
    "            # Get indices for this batch\n",
    "            batch_indices = indices_to_process[batch_idx:batch_idx + batch_size]\n",
    "            \n",
    "            # Process the batch\n",
    "            batch_results = analyze_sentiment_batch(batch_indices, filtered_news, retry_limit)\n",
    "            \n",
    "            # Update results and processed indices\n",
    "            results.update(batch_results)\n",
    "            for idx in batch_indices:\n",
    "                processed_indices.add(idx)\n",
    "            \n",
    "            # Save progress after each batch\n",
    "            save_processed_indices(progress_file, processed_indices)\n",
    "            \n",
    "            # Log progress\n",
    "            items_processed = min(batch_idx + batch_size, len(indices_to_process))\n",
    "            logger.info(f\"Processed {items_processed}/{len(indices_to_process)} items ({items_processed/len(indices_to_process)*100:.1f}%)\")\n",
    "        \n",
    "        # Create the result dataframe\n",
    "        rows = []\n",
    "        for idx, sentiment in results.items():\n",
    "            # Get original news item data\n",
    "            news_item = filtered_news[idx]\n",
    "            \n",
    "            # Create a row with required fields\n",
    "            row = {\n",
    "                'ticker': news_item['ticker'],\n",
    "                'date': news_item['clean_date'],\n",
    "                'title': news_item.get('title', ''),\n",
    "                'link': news_item.get('link', '')\n",
    "            }\n",
    "            \n",
    "            # Add sentiment scores\n",
    "            for key, value in sentiment.items():\n",
    "                row[key] = value\n",
    "            \n",
    "            rows.append(row)\n",
    "        \n",
    "        # Create dataframe from rows\n",
    "        if rows:\n",
    "            result_df = pd.DataFrame(rows)\n",
    "            \n",
    "            # Save results\n",
    "            output_file = os.path.join(output_dir, f\"news_sentiment_{ticker}.csv\")\n",
    "            result_df.to_csv(output_file, index=False)\n",
    "            logger.info(f\"Saved results to {output_file}\")\n",
    "        else:\n",
    "            logger.warning(f\"No results generated for {ticker}\")\n",
    "        \n",
    "        # Save progress\n",
    "        save_processed_indices(progress_file, processed_indices)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {file_path}: {e}\")\n",
    "        import traceback\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def find_all_news_files():\n",
    "    \"\"\"\n",
    "    Find all news JSON files in the data directory\n",
    "    \"\"\"\n",
    "    files = glob.glob(\"data/news_data/*_news.json\")\n",
    "    return files\n",
    "\n",
    "def process_multiple_news_files(files=None, num_rows=None, force_reprocess=True, batch_size=10):\n",
    "    \"\"\"\n",
    "    Process multiple news files, running sentiment analysis on each\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    files : list or None\n",
    "        List of file paths to process. If None, processes all news files\n",
    "    num_rows : int or None\n",
    "        Number of rows to process per file. If None, processes all rows\n",
    "    force_reprocess : bool\n",
    "        If True, reprocess all rows even if they've been processed before\n",
    "    batch_size : int\n",
    "        Number of news items to process in a batch\n",
    "    \"\"\"\n",
    "    if files is None:\n",
    "        # Find all news files\n",
    "        files = find_all_news_files()\n",
    "    \n",
    "    if not files:\n",
    "        logger.error(\"No news files found\")\n",
    "        return\n",
    "    \n",
    "    total_files = len(files)\n",
    "    logger.info(f\"Found {total_files} news files to process\")\n",
    "    \n",
    "    for i, file_path in enumerate(files):\n",
    "        logger.info(f\"Processing file {i+1}/{total_files}: {file_path}\")\n",
    "        process_news_file(file_path, num_rows=num_rows, force_reprocess=force_reprocess, batch_size=batch_size)\n",
    "    \n",
    "    logger.info(\"Completed processing all files\")\n",
    "\n",
    "def test_gemini_api():\n",
    "    \"\"\"Test the Gemini API with a small sample text\"\"\"\n",
    "    logger.info(\"Testing Gemini API with a small sample...\")\n",
    "    sample_text = \"The company reported strong revenue growth and exceeded analyst expectations for the quarter. However, challenges in the supply chain have impacted margins.\"\n",
    "    \n",
    "    try:\n",
    "        # Simple prompt for testing\n",
    "        prompt = \"Analyze this text for sentiment and respond with a single word: positive, negative, or neutral: \" + sample_text\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"API Test Response: {response.text}\")\n",
    "        logger.info(\"Gemini API test successful!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gemini API test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # First test if the API is working\n",
    "    if not test_gemini_api():\n",
    "        logger.error(\"Gemini API test failed. Please check your API key and connection.\")\n",
    "        return\n",
    "    \n",
    "    # Process all files\n",
    "    print(\"\\n=== FINANCIAL NEWS SENTIMENT ANALYSIS ===\")\n",
    "    \n",
    "    # Get batch size\n",
    "    batch_size_input = input(\"Enter batch size (default 10): \")\n",
    "    batch_size = int(batch_size_input) if batch_size_input.strip() else 10\n",
    "    \n",
    "    # Force reprocessing?\n",
    "    force_reprocess_input = input(\"Force reprocessing of all rows, even if previously processed? (y/n): \")\n",
    "    force_reprocess = force_reprocess_input.lower() == 'y'\n",
    "    \n",
    "    # Number of rows?\n",
    "    rows_input = input(\"How many rows per file? (Enter a number or 'all' for all rows): \")\n",
    "    rows_per_file = None if rows_input.lower() == 'all' else int(rows_input)\n",
    "    \n",
    "    # Process files\n",
    "    process_multiple_news_files(num_rows=rows_per_file, force_reprocess=force_reprocess, batch_size=batch_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a158d-2c05-412d-a522-8c6a2e63773f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAS Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
