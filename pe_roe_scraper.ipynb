{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c57fc8-f23e-4888-9285-e39ca9243aad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25638f8e-993d-4503-80c6-95001a1e1d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: 2025-04-22 13:41:11.973935\n",
      "Processing batch 1/6\n",
      "Processing AAPL...\n",
      "  Saved 62 PE data points for AAPL to data/AAPL_pe.csv\n",
      "  Found 62 PE data points for AAPL\n",
      "  Loaded 61 ROE data points for AAPL from data/AAPL_roe.csv\n",
      "  Found 61 ROE data points for AAPL\n",
      "Processing MSFT...\n",
      "  Loaded 62 PE data points for MSFT from data/MSFT_pe.csv\n",
      "  Found 62 PE data points for MSFT\n",
      "  Loaded 61 ROE data points for MSFT from data/MSFT_roe.csv\n",
      "  Found 61 ROE data points for MSFT\n",
      "Processing NVDA...\n",
      "  Loaded 63 PE data points for NVDA from data/NVDA_pe.csv\n",
      "  Found 63 PE data points for NVDA\n",
      "  Loaded 62 ROE data points for NVDA from data/NVDA_roe.csv\n",
      "  Found 62 ROE data points for NVDA\n",
      "Processing GOOGL...\n",
      "  Loaded 62 PE data points for GOOGL from data/GOOGL_pe.csv\n",
      "  Found 62 PE data points for GOOGL\n",
      "  Loaded 61 ROE data points for GOOGL from data/GOOGL_roe.csv\n",
      "  Found 61 ROE data points for GOOGL\n",
      "Processing AMZN...\n",
      "  Saved 62 PE data points for AMZN to data/AMZN_pe.csv\n",
      "  Found 62 PE data points for AMZN\n",
      "  Loaded 61 ROE data points for AMZN from data/AMZN_roe.csv\n",
      "  Found 61 ROE data points for AMZN\n",
      "Batch 1 completed. Waiting 120 seconds before next batch...\n",
      "Processing batch 2/6\n",
      "Processing META...\n",
      "  Loaded 49 PE data points for META from data/META_pe.csv\n",
      "  Found 49 PE data points for META\n",
      "  Loaded 57 ROE data points for META from data/META_roe.csv\n",
      "  Found 57 ROE data points for META\n",
      "Processing BRK.B...\n",
      "  Saved 62 PE data points for BRK.B to data/BRK.B_pe.csv\n",
      "  Found 62 PE data points for BRK.B\n",
      "  Saved 61 ROE data points for BRK.B to data/BRK.B_roe.csv\n",
      "  Found 61 ROE data points for BRK.B\n",
      "Processing AVGO...\n",
      "  Saved 60 PE data points for AVGO to data/AVGO_pe.csv\n",
      "  Found 60 PE data points for AVGO\n",
      "  Loaded 62 ROE data points for AVGO from data/AVGO_roe.csv\n",
      "  Found 62 ROE data points for AVGO\n",
      "Processing TSLA...\n",
      "  Loaded 57 PE data points for TSLA from data/TSLA_pe.csv\n",
      "  Found 57 PE data points for TSLA\n",
      "  Loaded 59 ROE data points for TSLA from data/TSLA_roe.csv\n",
      "  Found 59 ROE data points for TSLA\n",
      "Processing LLY...\n",
      "  Loaded 62 PE data points for LLY from data/LLY_pe.csv\n",
      "  Found 62 PE data points for LLY\n",
      "  Loaded 61 ROE data points for LLY from data/LLY_roe.csv\n",
      "  Found 61 ROE data points for LLY\n",
      "Batch 2 completed. Waiting 120 seconds before next batch...\n",
      "Processing batch 3/6\n",
      "Processing WMT...\n",
      "  Loaded 63 PE data points for WMT from data/WMT_pe.csv\n",
      "  Found 63 PE data points for WMT\n",
      "  Loaded 62 ROE data points for WMT from data/WMT_roe.csv\n",
      "  Found 62 ROE data points for WMT\n",
      "Processing JPM...\n",
      "  Loaded 63 PE data points for JPM from data/JPM_pe.csv\n",
      "  Found 63 PE data points for JPM\n",
      "  Loaded 62 ROE data points for JPM from data/JPM_roe.csv\n",
      "  Found 62 ROE data points for JPM\n",
      "Processing V...\n",
      "  Loaded 62 PE data points for V from data/V_pe.csv\n",
      "  Found 62 PE data points for V\n",
      "  Loaded 61 ROE data points for V from data/V_roe.csv\n",
      "  Found 61 ROE data points for V\n",
      "Processing MA...\n",
      "  Loaded 62 PE data points for MA from data/MA_pe.csv\n",
      "  Found 62 PE data points for MA\n",
      "  Loaded 61 ROE data points for MA from data/MA_roe.csv\n",
      "  Found 61 ROE data points for MA\n",
      "Processing XOM...\n",
      "  Loaded 62 PE data points for XOM from data/XOM_pe.csv\n",
      "  Found 62 PE data points for XOM\n",
      "  Loaded 61 ROE data points for XOM from data/XOM_roe.csv\n",
      "  Found 61 ROE data points for XOM\n",
      "Batch 3 completed. Waiting 120 seconds before next batch...\n",
      "Processing batch 4/6\n",
      "Processing COST...\n",
      "  Loaded 63 PE data points for COST from data/COST_pe.csv\n",
      "  Found 63 PE data points for COST\n",
      "  Loaded 62 ROE data points for COST from data/COST_roe.csv\n",
      "  Found 62 ROE data points for COST\n",
      "Processing UNH...\n",
      "  Loaded 63 PE data points for UNH from data/UNH_pe.csv\n",
      "  Found 63 PE data points for UNH\n",
      "  Loaded 62 ROE data points for UNH from data/UNH_roe.csv\n",
      "  Found 62 ROE data points for UNH\n",
      "Processing HD...\n",
      "  Loaded 63 PE data points for HD from data/HD_pe.csv\n",
      "  Found 63 PE data points for HD\n",
      "  Loaded 62 ROE data points for HD from data/HD_roe.csv\n",
      "  Found 62 ROE data points for HD\n",
      "Processing PG...\n",
      "  Loaded 62 PE data points for PG from data/PG_pe.csv\n",
      "  Found 62 PE data points for PG\n",
      "  Loaded 61 ROE data points for PG from data/PG_roe.csv\n",
      "  Found 61 ROE data points for PG\n",
      "Processing JNJ...\n",
      "  Loaded 63 PE data points for JNJ from data/JNJ_pe.csv\n",
      "  Found 63 PE data points for JNJ\n",
      "  Loaded 62 ROE data points for JNJ from data/JNJ_roe.csv\n",
      "  Found 62 ROE data points for JNJ\n",
      "Batch 4 completed. Waiting 120 seconds before next batch...\n",
      "Processing batch 5/6\n",
      "Processing ABBV...\n",
      "  Saved 47 PE data points for ABBV to data/ABBV_pe.csv\n",
      "  Found 47 PE data points for ABBV\n",
      "  Loaded 53 ROE data points for ABBV from data/ABBV_roe.csv\n",
      "  Found 53 ROE data points for ABBV\n",
      "Processing CRM...\n",
      "  Loaded 63 PE data points for CRM from data/CRM_pe.csv\n",
      "  Found 63 PE data points for CRM\n",
      "  Loaded 62 ROE data points for CRM from data/CRM_roe.csv\n",
      "  Found 62 ROE data points for CRM\n",
      "Processing BAC...\n",
      "  Saved 63 PE data points for BAC to data/BAC_pe.csv\n",
      "  Found 63 PE data points for BAC\n",
      "  Loaded 62 ROE data points for BAC from data/BAC_roe.csv\n",
      "  Found 62 ROE data points for BAC\n",
      "Processing ORCL...\n",
      "  Loaded 63 PE data points for ORCL from data/ORCL_pe.csv\n",
      "  Found 63 PE data points for ORCL\n",
      "  Loaded 62 ROE data points for ORCL from data/ORCL_roe.csv\n",
      "  Found 62 ROE data points for ORCL\n",
      "Processing MRK...\n",
      "  Loaded 62 PE data points for MRK from data/MRK_pe.csv\n",
      "  Found 62 PE data points for MRK\n",
      "  Loaded 61 ROE data points for MRK from data/MRK_roe.csv\n",
      "  Found 61 ROE data points for MRK\n",
      "Batch 5 completed. Waiting 120 seconds before next batch...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# List of tickers to process\n",
    "tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"NVDA\", \"GOOGL\", \"AMZN\", \"META\", \"BRK.B\", \"AVGO\", \"TSLA\", \"LLY\",\n",
    "    \"WMT\", \"JPM\", \"V\", \"MA\", \"XOM\", \"COST\", \"UNH\", \"HD\", \"PG\", \"JNJ\",\n",
    "    \"ABBV\", \"CRM\", \"BAC\", \"ORCL\", \"MRK\", \"CVX\", \"WFC\", \"KO\", \"CSCO\", \"ACN\"\n",
    "]\n",
    "\n",
    "# Create a dict to map tickers to company names\n",
    "ticker_to_company = {\n",
    "    \"AAPL\": \"apple\", \"MSFT\": \"microsoft\", \"NVDA\": \"nvidia\", \"GOOGL\": \"alphabet\",\n",
    "    \"AMZN\": \"amazon\", \"META\": \"meta-platforms\", \"BRK.B\": \"berkshire-hathaway\",\n",
    "    \"AVGO\": \"broadcom\", \"TSLA\": \"tesla\", \"LLY\": \"eli-lilly\", \"WMT\": \"walmart\",\n",
    "    \"JPM\": \"jpmorgan-chase\", \"V\": \"visa\", \"MA\": \"mastercard\", \"XOM\": \"exxon-mobil\",\n",
    "    \"COST\": \"costco\", \"UNH\": \"unitedhealth-group\", \"HD\": \"home-depot\",\n",
    "    \"PG\": \"procter-gamble\", \"JNJ\": \"johnson-johnson\", \"ABBV\": \"abbvie\",\n",
    "    \"CRM\": \"salesforce\", \"BAC\": \"bank-of-america\", \"ORCL\": \"oracle\",\n",
    "    \"MRK\": \"merck\", \"CVX\": \"chevron\", \"WFC\": \"wells-fargo\", \"KO\": \"coca-cola\",\n",
    "    \"CSCO\": \"cisco-systems\", \"ACN\": \"accenture\"\n",
    "}\n",
    "\n",
    "# Function to get company name from ticker\n",
    "def get_company_name(ticker):\n",
    "    return ticker_to_company.get(ticker, ticker.lower())\n",
    "\n",
    "# Create a session for better performance and cookie handling\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "})\n",
    "\n",
    "# Create directory for progress tracking\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Function to save progress\n",
    "def save_progress(ticker, metric_type, data):\n",
    "    if data is not None and not data.empty:\n",
    "        filename = f\"data/{ticker}_{metric_type.lower()}.csv\"\n",
    "        data.to_csv(filename, index=False)\n",
    "        print(f\"  Saved {len(data)} {metric_type} data points for {ticker} to {filename}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to load progress\n",
    "def load_progress(ticker, metric_type):\n",
    "    filename = f\"data/{ticker}_{metric_type.lower()}.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            data = pd.read_csv(filename)\n",
    "            print(f\"  Loaded {len(data)} {metric_type} data points for {ticker} from {filename}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {filename}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Function for exponential backoff\n",
    "def make_request_with_backoff(url, max_retries=5, initial_delay=5):\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "    \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = session.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"  Rate limited (429). Backing off for {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"  HTTP error: {response.status_code}\")\n",
    "                return response\n",
    "        except Exception as e:\n",
    "            print(f\"  Request error: {e}\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "            retries += 1\n",
    "    \n",
    "    print(\"  Max retries exceeded\")\n",
    "    return None\n",
    "\n",
    "# Function to scrape PE ratio data\n",
    "def scrape_pe_ratio(ticker):\n",
    "    # First check if we already have data for this ticker\n",
    "    existing_data = load_progress(ticker, 'PE')\n",
    "    if existing_data is not None:\n",
    "        return existing_data\n",
    "    \n",
    "    company_name = get_company_name(ticker)\n",
    "    url = f\"https://www.macrotrends.net/stocks/charts/{ticker}/{company_name}/pe-ratio\"\n",
    "    \n",
    "    response = make_request_with_backoff(url)\n",
    "    if response is None or response.status_code != 200:\n",
    "        print(f\"  Failed to fetch PE data for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Look for div with style-1 ID\n",
    "        div = soup.find('div', id='style-1')\n",
    "        if not div:\n",
    "            print(f\"  Could not find style-1 div for {ticker} PE data\")\n",
    "            return None\n",
    "        \n",
    "        # Find the table within this div\n",
    "        table = div.find('table', class_='table')\n",
    "        if not table:\n",
    "            print(f\"  Could not find table in style-1 div for {ticker} PE data\")\n",
    "            return None\n",
    "        \n",
    "        # Check if this is the PE ratio table\n",
    "        title_row = table.find('th', string=re.compile('PE Ratio'))\n",
    "        if not title_row:\n",
    "            print(f\"  Table does not appear to be PE ratio table for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data from the table\n",
    "        pe_data = []\n",
    "        for row in table.find_all('tr')[2:]:  # Skip the first two header rows\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 4:\n",
    "                try:\n",
    "                    date = cells[0].text.strip()\n",
    "                    stock_price = cells[1].text.strip()\n",
    "                    eps = cells[2].text.strip()\n",
    "                    pe_ratio = cells[3].text.strip()\n",
    "                    \n",
    "                    # Only include rows with valid PE data\n",
    "                    if pe_ratio and pe_ratio != \"N/A\":\n",
    "                        pe_data.append({\n",
    "                            'Ticker': ticker,\n",
    "                            'Date': date,\n",
    "                            'Stock_Price': stock_price,\n",
    "                            'EPS': eps,\n",
    "                            'PE_Ratio': pe_ratio\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error parsing row for {ticker} PE data: {e}\")\n",
    "        \n",
    "        result_df = pd.DataFrame(pe_data)\n",
    "        if not result_df.empty:\n",
    "            save_progress(ticker, 'PE', result_df)\n",
    "        return result_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error scraping PE data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to scrape ROE data\n",
    "def scrape_roe(ticker):\n",
    "    # First check if we already have data for this ticker\n",
    "    existing_data = load_progress(ticker, 'ROE')\n",
    "    if existing_data is not None:\n",
    "        return existing_data\n",
    "    \n",
    "    company_name = get_company_name(ticker)\n",
    "    url = f\"https://www.macrotrends.net/stocks/charts/{ticker}/{company_name}/roe\"\n",
    "    \n",
    "    response = make_request_with_backoff(url)\n",
    "    if response is None or response.status_code != 200:\n",
    "        print(f\"  Failed to fetch ROE data for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Look for div with style-1 ID\n",
    "        div = soup.find('div', id='style-1')\n",
    "        if not div:\n",
    "            print(f\"  Could not find style-1 div for {ticker} ROE data\")\n",
    "            return None\n",
    "        \n",
    "        # Find the table within this div\n",
    "        table = div.find('table', class_='table')\n",
    "        if not table:\n",
    "            print(f\"  Could not find table in style-1 div for {ticker} ROE data\")\n",
    "            return None\n",
    "        \n",
    "        # Check if this is the ROE table\n",
    "        title_row = table.find('th', string=re.compile('Return on Equity'))\n",
    "        if not title_row:\n",
    "            print(f\"  Table does not appear to be ROE table for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data from the table\n",
    "        roe_data = []\n",
    "        for row in table.find_all('tr')[2:]:  # Skip the first two header rows\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 4:\n",
    "                try:\n",
    "                    date = cells[0].text.strip()\n",
    "                    net_income = cells[1].text.strip()\n",
    "                    equity = cells[2].text.strip()\n",
    "                    roe = cells[3].text.strip()\n",
    "                    \n",
    "                    # Only include rows with valid ROE data\n",
    "                    if roe and roe != \"N/A\":\n",
    "                        roe_data.append({\n",
    "                            'Ticker': ticker,\n",
    "                            'Date': date,\n",
    "                            'Net_Income': net_income,\n",
    "                            'Shareholder_Equity': equity,\n",
    "                            'ROE': roe\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error parsing row for {ticker} ROE data: {e}\")\n",
    "        \n",
    "        result_df = pd.DataFrame(roe_data)\n",
    "        if not result_df.empty:\n",
    "            save_progress(ticker, 'ROE', result_df)\n",
    "        return result_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error scraping ROE data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process tickers in batches\n",
    "def process_in_batches(tickers_list, batch_size=5, batch_delay=60):\n",
    "    pe_dataframes = []\n",
    "    roe_dataframes = []\n",
    "    \n",
    "    # Break the list into batches\n",
    "    batches = [tickers_list[i:i + batch_size] for i in range(0, len(tickers_list), batch_size)]\n",
    "    total_batches = len(batches)\n",
    "    \n",
    "    for batch_num, batch in enumerate(batches, 1):\n",
    "        print(f\"Processing batch {batch_num}/{total_batches}\")\n",
    "        \n",
    "        for ticker in batch:\n",
    "            print(f\"Processing {ticker}...\")\n",
    "            \n",
    "            # Scrape PE data\n",
    "            pe_df = scrape_pe_ratio(ticker)\n",
    "            if pe_df is not None and not pe_df.empty:\n",
    "                pe_dataframes.append(pe_df)\n",
    "                print(f\"  Found {len(pe_df)} PE data points for {ticker}\")\n",
    "            else:\n",
    "                print(f\"  No PE data found for {ticker}\")\n",
    "            \n",
    "            # Add random delay between requests for same ticker\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "            \n",
    "            # Scrape ROE data\n",
    "            roe_df = scrape_roe(ticker)\n",
    "            if roe_df is not None and not roe_df.empty:\n",
    "                roe_dataframes.append(roe_df)\n",
    "                print(f\"  Found {len(roe_df)} ROE data points for {ticker}\")\n",
    "            else:\n",
    "                print(f\"  No ROE data found for {ticker}\")\n",
    "            \n",
    "            # Add random delay between tickers\n",
    "            time.sleep(random.uniform(10, 15))\n",
    "        \n",
    "        if batch_num < total_batches:\n",
    "            print(f\"Batch {batch_num} completed. Waiting {batch_delay} seconds before next batch...\")\n",
    "            time.sleep(batch_delay)\n",
    "    \n",
    "    return pe_dataframes, roe_dataframes\n",
    "\n",
    "# Function to merge all data files\n",
    "def merge_progress_files():\n",
    "    pe_files = [f for f in os.listdir('data') if f.endswith('_pe.csv')]\n",
    "    roe_files = [f for f in os.listdir('data') if f.endswith('_roe.csv')]\n",
    "    \n",
    "    pe_dataframes = []\n",
    "    roe_dataframes = []\n",
    "    \n",
    "    for file in pe_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"data/{file}\")\n",
    "            pe_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    for file in roe_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"data/{file}\")\n",
    "            roe_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    pe_data = pd.concat(pe_dataframes, ignore_index=True) if pe_dataframes else pd.DataFrame()\n",
    "    roe_data = pd.concat(roe_dataframes, ignore_index=True) if roe_dataframes else pd.DataFrame()\n",
    "    \n",
    "    return pe_data, roe_data\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Script started at: {start_time}\")\n",
    "    \n",
    "    # Process tickers in batches\n",
    "    pe_dataframes, roe_dataframes = process_in_batches(tickers, batch_size=5, batch_delay=120)\n",
    "    \n",
    "    # Alternatively, merge from progress files\n",
    "    if not pe_dataframes or not roe_dataframes:\n",
    "        print(\"Merging progress files...\")\n",
    "        pe_data, roe_data = merge_progress_files()\n",
    "    else:\n",
    "        # Combine all data\n",
    "        pe_data = pd.concat(pe_dataframes, ignore_index=True) if pe_dataframes else pd.DataFrame()\n",
    "        roe_data = pd.concat(roe_dataframes, ignore_index=True) if roe_dataframes else pd.DataFrame()\n",
    "    \n",
    "    # Save to CSV files\n",
    "    if not pe_data.empty:\n",
    "        pe_data.to_csv('pe_ratios.csv', index=False)\n",
    "        print(\"PE ratio data saved to pe_ratios.csv\")\n",
    "        print(f\"Total tickers with PE data: {pe_data['Ticker'].nunique()}\")\n",
    "        print(f\"Total PE data points: {len(pe_data)}\")\n",
    "    else:\n",
    "        print(\"No PE ratio data was collected\")\n",
    "    \n",
    "    if not roe_data.empty:\n",
    "        roe_data.to_csv('roe_data.csv', index=False)\n",
    "        print(\"ROE data saved to roe_data.csv\")\n",
    "        print(f\"Total tickers with ROE data: {roe_data['Ticker'].nunique()}\")\n",
    "        print(f\"Total ROE data points: {len(roe_data)}\")\n",
    "    else:\n",
    "        print(\"No ROE data was collected\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Script completed at: {end_time}\")\n",
    "    print(f\"Total runtime: {end_time - start_time}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa843e82-f222-4f8e-90d7-d8cdded98603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Create a session for better performance and cookie handling\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0'\n",
    "})\n",
    "\n",
    "# Create directory for progress tracking\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Function to save progress\n",
    "def save_progress(ticker, metric_type, data):\n",
    "    if data is not None and not data.empty:\n",
    "        filename = f\"data/{ticker}_{metric_type.lower()}.csv\"\n",
    "        data.to_csv(filename, index=False)\n",
    "        print(f\"  Saved {len(data)} {metric_type} data points for {ticker} to {filename}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function for exponential backoff\n",
    "def make_request_with_backoff(url, max_retries=5, initial_delay=5):\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "    \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = session.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"  Rate limited (429). Backing off for {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"  HTTP error: {response.status_code}\")\n",
    "                return response\n",
    "        except Exception as e:\n",
    "            print(f\"  Request error: {e}\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "            retries += 1\n",
    "    \n",
    "    print(\"  Max retries exceeded\")\n",
    "    return None\n",
    "\n",
    "# Function to scrape PE ratio data for BRK.B\n",
    "def scrape_pe_for_brkb():\n",
    "    ticker = \"BRK-B\"\n",
    "    \n",
    "    print(f\"Attempting to scrape PE ratio data for {ticker}...\")\n",
    "    \n",
    "    # Use the specific URL provided\n",
    "    url = \"https://www.macrotrends.net/stocks/charts/BRK.B/berkshire-hathaway/pe-ratio\"\n",
    "    \n",
    "    print(f\"  Trying URL: {url}\")\n",
    "    response = make_request_with_backoff(url)\n",
    "    \n",
    "    if response is None or response.status_code != 200:\n",
    "        print(f\"  Failed to fetch PE data for {ticker} using {url}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Look for div with style-1 ID\n",
    "        div = soup.find('div', id='style-1')\n",
    "        if not div:\n",
    "            print(f\"  Could not find style-1 div for {ticker} PE data\")\n",
    "            return None\n",
    "        \n",
    "        # Find the table within this div\n",
    "        table = div.find('table', class_='table')\n",
    "        if not table:\n",
    "            print(f\"  Could not find table in style-1 div for {ticker} PE data\")\n",
    "            return None\n",
    "        \n",
    "        # Check if this is the PE ratio table\n",
    "        title_row = table.find('th', string=re.compile('PE Ratio'))\n",
    "        if not title_row:\n",
    "            print(f\"  Table does not appear to be PE ratio table for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data from the table\n",
    "        pe_data = []\n",
    "        for row in table.find_all('tr')[2:]:  # Skip the first two header rows\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 4:\n",
    "                try:\n",
    "                    date = cells[0].text.strip()\n",
    "                    stock_price = cells[1].text.strip()\n",
    "                    eps = cells[2].text.strip()\n",
    "                    pe_ratio = cells[3].text.strip()\n",
    "                    \n",
    "                    # Only include rows with valid PE data\n",
    "                    if pe_ratio and pe_ratio != \"N/A\":\n",
    "                        pe_data.append({\n",
    "                            'Ticker': ticker,\n",
    "                            'Date': date,\n",
    "                            'Stock_Price': stock_price,\n",
    "                            'EPS': eps,\n",
    "                            'PE_Ratio': pe_ratio\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error parsing row for {ticker} PE data: {e}\")\n",
    "        \n",
    "        result_df = pd.DataFrame(pe_data)\n",
    "        if not result_df.empty:\n",
    "            save_progress(ticker, 'PE', result_df)\n",
    "            # Also save to a dedicated file\n",
    "            result_df.to_csv('brk_b_pe_data.csv', index=False)\n",
    "            print(f\"  Successfully scraped {len(result_df)} PE data points for {ticker}\")\n",
    "            print(f\"  Data saved to brk_b_pe_data.csv\")\n",
    "            return result_df\n",
    "        else:\n",
    "            print(f\"  No PE data found for {ticker} using {url}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error scraping PE data for {ticker} using {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to scrape ROE data for BRK.B\n",
    "def scrape_roe_for_brkb():\n",
    "    ticker = \"BRK-B\"\n",
    "    company_name = \"berkshire-hathaway\"\n",
    "    \n",
    "    print(f\"Attempting to scrape ROE data for {ticker}...\")\n",
    "    \n",
    "    # Try both URL formats - with dash and with dot\n",
    "    urls = [\n",
    "        f\"https://www.macrotrends.net/stocks/charts/{ticker}/{company_name}/roe\",\n",
    "        f\"https://www.macrotrends.net/stocks/charts/BRK.B/{company_name}/roe\"\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"  Trying URL: {url}\")\n",
    "        response = make_request_with_backoff(url)\n",
    "        \n",
    "        if response is None or response.status_code != 200:\n",
    "            print(f\"  Failed to fetch ROE data for {ticker} using {url}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for div with style-1 ID\n",
    "            div = soup.find('div', id='style-1')\n",
    "            if not div:\n",
    "                print(f\"  Could not find style-1 div for {ticker} ROE data\")\n",
    "                continue\n",
    "            \n",
    "            # Find the table within this div\n",
    "            table = div.find('table', class_='table')\n",
    "            if not table:\n",
    "                print(f\"  Could not find table in style-1 div for {ticker} ROE data\")\n",
    "                continue\n",
    "            \n",
    "            # Check if this is the ROE table\n",
    "            title_row = table.find('th', string=re.compile('Return on Equity'))\n",
    "            if not title_row:\n",
    "                print(f\"  Table does not appear to be ROE table for {ticker}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract data from the table\n",
    "            roe_data = []\n",
    "            for row in table.find_all('tr')[2:]:  # Skip the first two header rows\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 4:\n",
    "                    try:\n",
    "                        date = cells[0].text.strip()\n",
    "                        net_income = cells[1].text.strip()\n",
    "                        equity = cells[2].text.strip()\n",
    "                        roe = cells[3].text.strip()\n",
    "                        \n",
    "                        # Only include rows with valid ROE data\n",
    "                        if roe and roe != \"N/A\":\n",
    "                            roe_data.append({\n",
    "                                'Ticker': ticker,\n",
    "                                'Date': date,\n",
    "                                'Net_Income': net_income,\n",
    "                                'Shareholder_Equity': equity,\n",
    "                                'ROE': roe\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error parsing row for {ticker} ROE data: {e}\")\n",
    "            \n",
    "            result_df = pd.DataFrame(roe_data)\n",
    "            if not result_df.empty:\n",
    "                save_progress(ticker, 'ROE', result_df)\n",
    "                # Also save to a dedicated file\n",
    "                result_df.to_csv('brk_b_roe_data.csv', index=False)\n",
    "                print(f\"  Successfully scraped {len(result_df)} ROE data points for {ticker}\")\n",
    "                print(f\"  Data saved to brk_b_roe_data.csv\")\n",
    "                return result_df\n",
    "            else:\n",
    "                print(f\"  No ROE data found for {ticker} using {url}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Error scraping ROE data for {ticker} using {url}: {e}\")\n",
    "    \n",
    "    print(f\"  Could not retrieve ROE data for {ticker} after trying all URLs\")\n",
    "    return None\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Starting BRK.B data retrieval...\")\n",
    "    \n",
    "    # Add a delay between requests to avoid rate limiting\n",
    "    pe_data = scrape_pe_for_brkb()\n",
    "    time.sleep(10)  # Wait 10 seconds between requests\n",
    "    roe_data = scrape_roe_for_brkb()\n",
    "    \n",
    "    # Summarize results\n",
    "    print(\"\\nResults summary:\")\n",
    "    \n",
    "    if pe_data is not None and not pe_data.empty:\n",
    "        print(f\"✅ Successfully retrieved {len(pe_data)} PE data points for BRK.B\")\n",
    "    else:\n",
    "        print(\"❌ Failed to retrieve PE data for BRK.B\")\n",
    "    \n",
    "    if roe_data is not None and not roe_data.empty:\n",
    "        print(f\"✅ Successfully retrieved {len(roe_data)} ROE data points for BRK.B\")\n",
    "    else:\n",
    "        print(\"❌ Failed to retrieve ROE data for BRK.B\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAS Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
